(llama_hw) PS C:\Users\hongf\miniconda3\envs\Build_Your_Own_LLaMa-Practice\minllama-assignment-ma(llama_hw) PS C:\Users\hongf\miniconda3\envs\Build_Your_Own_LLaMa-Practice\minllama-assignment-master> python run_llama.py --option pretrain --pretrain_data data/sst-train.txt --pretrain_batch_size 8 --gradient_accumulation_steps 1 --epochs 100 --pretrain_lr 1e-4  --warmup_steps 10 --min_lr 1e-7 --lr_scheduler cosine --save_interval 50 --best_model_interval 50 --use_gpu
args: {'train': 'data/cfimdb-train.txt', 'dev': 'data/cfimdb-dev.txt', 'test': 'data/cfimdb-test.txt', 'label_names': 'data/cfimdb-label-mapping.json', 'pretrained_model_path': 'stories42M.pt', 'max_sentence_len': None, 'seed': 1337, 'epochs': 100, 'option': 'pretrain', 'use_gpu': True, 'generated_sentence_low_temp_out': 'generated-sentence-temp-0.txt', 'generated_sentence_high_temp_out': 'generated-sentence-temp-1.txt', 'dev_out': 'cfimdb-dev-prompting-output.txt', 'test_out': 'cfimdb-test-prompting-output.txt', 'batch_size': 8, 'hidden_dropout_prob': 0.3, 'lr': 2e-05, 'pretrain_data': 'data/sst-train.txt', 'max_seq_len': 1024, 'pretrain_batch_size': 8, 'pretrain_lr': 0.0001, 'warmup_steps': 10, 'warmup_ratio': 0.01, 'min_lr': 1e-07, 'lr_scheduler': 'cosine', 'weight_decay': 0.01, 'gradient_accumulation_steps': 1, 'checkpoint_interval': 10000, 'save_interval': 50, 'best_model_interval': 50, 'save_dir': './checkpoints'}
Using device: cuda
Loaded 1 chunks from data/sst-train.txt
Epoch 1/100: 100%|████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.20s/it]
Epoch 1/100: Loss = 10.4963, LR = 0.000011
Epoch 2/100: 100%|████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.08s/it]
Epoch 2/100: Loss = 10.4358, LR = 0.000021
Epoch 3/100: 100%|████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.07s/it]
Epoch 3/100: Loss = 10.0012, LR = 0.000031
Epoch 4/100: 100%|████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.92s/it]
Epoch 4/100: Loss = 9.7490, LR = 0.000041
Epoch 5/100: 100%|████████████████████████████████████████████████| 1/1 [00:06<00:00,  7.00s/it]
Epoch 5/100: Loss = 9.5760, LR = 0.000051
Epoch 6/100: 100%|████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.01s/it]
Epoch 6/100: Loss = 9.4542, LR = 0.000060
Epoch 7/100: 100%|████████████████████████████████████████████████| 1/1 [00:06<00:00,  7.00s/it] 
Epoch 7/100: Loss = 9.3812, LR = 0.000070
Epoch 8/100: 100%|████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.98s/it] 
Epoch 8/100: Loss = 9.3345, LR = 0.000080
Epoch 9/100: 100%|████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.06s/it] 
Epoch 9/100: Loss = 9.2938, LR = 0.000090
Epoch 10/100: 100%|███████████████████████████████████████████████| 1/1 [00:06<00:00,  6.90s/it] 
Epoch 10/100: Loss = 9.2519, LR = 0.000100
Epoch 11/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.08s/it] 
Epoch 11/100: Loss = 9.1969, LR = 0.000100
Epoch 11/100: Loss = 9.1969, LR = 0.000100
Epoch 12/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.14s/it] 
Epoch 12/100: Loss = 9.1373, LR = 0.000100
Epoch 13/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.04s/it] 
Epoch 13/100: Loss = 9.0769, LR = 0.000100
Epoch 14/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.16s/it] 
Epoch 14/100: Loss = 9.0071, LR = 0.000100
Epoch 15/100: 100%|███████████████████████████████████████████████| 1/1 [00:06<00:00,  6.99s/it] 
Epoch 15/100: Loss = 8.9440, LR = 0.000099
Epoch 16/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.04s/it] 
Epoch 16/100: Loss = 8.8788, LR = 0.000099
Epoch 17/100: 100%|███████████████████████████████████████████████| 1/1 [00:06<00:00,  6.97s/it] 
Epoch 17/100: Loss = 8.8128, LR = 0.000099
Epoch 18/100: 100%|███████████████████████████████████████████████| 1/1 [00:06<00:00,  6.98s/it] 
Epoch 18/100: Loss = 8.7429, LR = 0.000098
Epoch 19/100: 100%|███████████████████████████████████████████████| 1/1 [00:06<00:00,  6.98s/it] 
Epoch 19/100: Loss = 8.6710, LR = 0.000098
Epoch 20/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 20/100: Loss = 8.6042, LR = 0.000097
Epoch 18/100: Loss = 8.7429, LR = 0.000098
Epoch 19/100: 100%|███████████████████████████████████████████████| 1/1 [00:06<00:00,  6.98s/it] 
Epoch 19/100: Loss = 8.6710, LR = 0.000098
Epoch 20/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 20/100: Loss = 8.6042, LR = 0.000097
Epoch 19/100: 100%|███████████████████████████████████████████████| 1/1 [00:06<00:00,  6.98s/it] 
Epoch 19/100: Loss = 8.6710, LR = 0.000098
Epoch 20/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 20/100: Loss = 8.6042, LR = 0.000097
Epoch 19/100: Loss = 8.6710, LR = 0.000098
Epoch 20/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 20/100: Loss = 8.6042, LR = 0.000097
Epoch 21/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.04s/it] 
Epoch 20/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 20/100: Loss = 8.6042, LR = 0.000097
Epoch 21/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.04s/it] 
Epoch 21/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.04s/it] 
Epoch 21/100: Loss = 8.5358, LR = 0.000096
Epoch 21/100: Loss = 8.5358, LR = 0.000096
Epoch 22/100: 100%|███████████████████████████████████████████████| 1/1 [00:06<00:00,  6.99s/it] 
Epoch 22/100: Loss = 8.4606, LR = 0.000096
Epoch 23/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.43s/it] 
Epoch 23/100: Loss = 8.3862, LR = 0.000095
Epoch 24/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.41s/it] 
Epoch 24/100: Loss = 8.3142, LR = 0.000094
Epoch 25/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.40s/it] 
Epoch 25/100: Loss = 8.2445, LR = 0.000093
Epoch 26/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.18s/it] 
Epoch 26/100: Loss = 8.1705, LR = 0.000092
Epoch 27/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.43s/it] 
Epoch 27/100: Loss = 8.0990, LR = 0.000091
Epoch 28/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.14s/it] 
Epoch 28/100: Loss = 8.0317, LR = 0.000090
Epoch 29/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it] 
Epoch 29/100: Loss = 7.9579, LR = 0.000089
Epoch 30/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 30/100: Loss = 7.8852, LR = 0.000088
Epoch 31/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.02s/it] 
Epoch 31/100: Loss = 7.8157, LR = 0.000087
Epoch 32/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.23s/it] 
Epoch 32/100: Loss = 7.7431, LR = 0.000086
Epoch 33/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.44s/it] 
Epoch 33/100: Loss = 7.6741, LR = 0.000085
Epoch 34/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.31s/it] 
Epoch 34/100: Loss = 7.6074, LR = 0.000083
Epoch 35/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.26s/it] 
Epoch 35/100: Loss = 7.5392, LR = 0.000082
Epoch 36/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.24s/it] 
Epoch 36/100: Loss = 7.4750, LR = 0.000081
Epoch 36/100: Loss = 7.4750, LR = 0.000081
Epoch 37/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.37s/it] 
Epoch 37/100: Loss = 7.4095, LR = 0.000079
Epoch 38/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.10s/it] 
Epoch 38/100: Loss = 7.3445, LR = 0.000078
Epoch 39/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.22s/it] 
Epoch 39/100: Loss = 7.2765, LR = 0.000077
Epoch 40/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it] 
Epoch 40/100: Loss = 7.2211, LR = 0.000075
Epoch 41/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.24s/it] 
Epoch 41/100: Loss = 7.1615, LR = 0.000074
Epoch 42/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 42/100: Loss = 7.1020, LR = 0.000072
Epoch 43/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 43/100: Loss = 7.0417, LR = 0.000070
Epoch 44/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.20s/it] 
Epoch 44/100: Loss = 6.9847, LR = 0.000069
Epoch 45/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.16s/it] 
Epoch 45/100: Loss = 6.9272, LR = 0.000067
Epoch 46/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.26s/it] 
Epoch 46/100: Loss = 6.8718, LR = 0.000065
Epoch 47/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.14s/it] 
Epoch 47/100: Loss = 6.8152, LR = 0.000064
Epoch 48/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.67s/it] 
Epoch 48/100: Loss = 6.7564, LR = 0.000062
Epoch 49/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.34s/it]
Epoch 49/100: Loss = 6.6950, LR = 0.000060
Epoch 50/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 50/100: Loss = 6.6662, LR = 0.000059
save the model to ./checkpoints\best-model-epoch-50.pt
save the model to ./checkpoints\model-epoch-50.pt
Epoch 51/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.23s/it]
Epoch 51/100: Loss = 6.5799, LR = 0.000057
Epoch 52/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.14s/it] 
Epoch 52/100: Loss = 14.3297, LR = 0.000055
Epoch 53/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.25s/it] 
Epoch 53/100: Loss = 14.3168, LR = 0.000054
Epoch 54/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it] 
Epoch 54/100: Loss = 14.2259, LR = 0.000052
Epoch 55/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.13s/it] 
Epoch 55/100: Loss = 14.0338, LR = 0.000050
Epoch 56/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.06s/it] 
Epoch 56/100: Loss = 13.6626, LR = 0.000048
Epoch 57/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it] 
Epoch 57/100: Loss = 12.9359, LR = 0.000047
Epoch 58/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 58/100: Loss = 10.7591, LR = 0.000045
Epoch 59/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.28s/it] 
Epoch 59/100: Loss = 6.8422, LR = 0.000043
Epoch 60/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.34s/it] 
Epoch 60/100: Loss = 6.6384, LR = 0.000041
Epoch 61/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it] 
Epoch 61/100: Loss = 6.6506, LR = 0.000040
Epoch 62/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.24s/it] 
Epoch 62/100: Loss = 6.6270, LR = 0.000038
Epoch 63/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.39s/it] 
Epoch 63/100: Loss = 6.5786, LR = 0.000036
Epoch 64/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.16s/it] 
Epoch 64/100: Loss = 6.5237, LR = 0.000035
Epoch 65/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 65/100: Loss = 6.4905, LR = 0.000033
Epoch 66/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it] 
Epoch 66/100: Loss = 6.4769, LR = 0.000031
Epoch 67/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 67/100: Loss = 6.4674, LR = 0.000030
Epoch 68/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.42s/it] 
Epoch 68/100: Loss = 6.4561, LR = 0.000028
Epoch 69/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.58s/it] 
Epoch 69/100: Loss = 6.4341, LR = 0.000027
Epoch 70/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.18s/it] 
Epoch 70/100: Loss = 6.4185, LR = 0.000025
Epoch 71/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 71/100: Loss = 6.3988, LR = 0.000024
Epoch 72/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 72/100: Loss = 6.3841, LR = 0.000022
Epoch 73/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.20s/it] 
Epoch 73/100: Loss = 6.3782, LR = 0.000021
Epoch 74/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.14s/it] 
Epoch 74/100: Loss = 6.3652, LR = 0.000019
Epoch 75/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.04s/it] 
Epoch 75/100: Loss = 6.3542, LR = 0.000018
Epoch 76/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 76/100: Loss = 6.3354, LR = 0.000017
Epoch 77/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 77/100: Loss = 6.3270, LR = 0.000015
Epoch 78/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 78/100: Loss = 6.3153, LR = 0.000014
Epoch 79/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.18s/it] 
Epoch 79/100: Loss = 6.3098, LR = 0.000013
Epoch 80/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.18s/it] 
Epoch 80/100: Loss = 6.2940, LR = 0.000012
Epoch 81/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 81/100: Loss = 6.2888, LR = 0.000011
Epoch 81/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 81/100: Loss = 6.2888, LR = 0.000011
Epoch 81/100: Loss = 6.2888, LR = 0.000011
Epoch 82/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it] 
Epoch 82/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it] 
Epoch 82/100: Loss = 6.2813, LR = 0.000010
Epoch 83/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 83/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.21s/it] 
Epoch 83/100: Loss = 6.2769, LR = 0.000009
Epoch 84/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.27s/it] 
Epoch 84/100: Loss = 6.2684, LR = 0.000008
Epoch 85/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.12s/it] 
Epoch 85/100: Loss = 6.2617, LR = 0.000007
Epoch 86/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it] 
Epoch 86/100: Loss = 6.2559, LR = 0.000006
Epoch 87/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.07s/it] 
Epoch 87/100: Loss = 6.2549, LR = 0.000005
Epoch 88/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.15s/it] 
Epoch 88/100: Loss = 6.2492, LR = 0.000004
Epoch 89/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.24s/it] 
Epoch 89/100: Loss = 6.2437, LR = 0.000004
Epoch 90/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.19s/it] 
Epoch 90/100: Loss = 6.2427, LR = 0.000003
Epoch 91/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.41s/it] 
Epoch 91/100: Loss = 6.2406, LR = 0.000003
Epoch 92/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.60s/it] 
Epoch 92/100: Loss = 6.2355, LR = 0.000002
Epoch 93/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.37s/it] 
Epoch 93/100: Loss = 6.2368, LR = 0.000002
Epoch 94/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.43s/it] 
Epoch 94/100: Loss = 6.2327, LR = 0.000001
Epoch 95/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.36s/it] 
Epoch 95/100: Loss = 6.2347, LR = 0.000001
Epoch 96/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.42s/it] 
Epoch 96/100: Loss = 6.2302, LR = 0.000001
Epoch 97/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.60s/it] 
Epoch 97/100: Loss = 6.2335, LR = 0.000000
Epoch 98/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.61s/it]
Epoch 98/100: Loss = 6.2339, LR = 0.000000
Epoch 99/100: 100%|███████████████████████████████████████████████| 1/1 [00:07<00:00,  7.36s/it]
Epoch 99/100: Loss = 6.2317, LR = 0.000000
Epoch 100/100: 100%|██████████████████████████████████████████████| 1/1 [00:07<00:00,  7.40s/it]
Epoch 100/100: Loss = 6.2315, LR = 0.000000
save the model to ./checkpoints\best-model-epoch-100.pt
save the model to ./checkpoints\model-epoch-100.pt
save the model to ./checkpoints\final-model.pt
Pretraining completed. Final model saved to ./checkpoints\final-model.pt




####

根据你提供的训练日志（ strangeResult.md ），我发现了几个异常现象：

### 1. 训练过程中的异常
- 损失值突变 ：在第 52 个 epoch 时，损失值从 ~6.58 突然跃升至 ~14.33，随后又逐渐下降。
- 重复日志 ：多个 epoch 出现了重复的日志输出（例如第 11、18-21 等 epoch）。
- 数据量极小 ：日志显示 Loaded 1 chunks from data/sst-train.txt ，说明只加载了 1 个数据块进行训练。
### 2. 可能的原因
1. 数据问题 ： pretrain_data 使用了 data/sst-train.txt （情感分析数据集），而非专门的预训练语料库，数据量过小且格式可能不适合自回归预训练。
2. 学习率调度 ：虽然使用了余弦退火调度，但在第 50 个 epoch 后学习率已降至较低水平（~0.000059），可能导致模型陷入局部最优后跳变。
3. 代码逻辑 ： run_llama.py 中的 pretrain 函数可能存在日志重复输出的问题（例如 print 语句被多次执行）。
### 3. 建议的解决方案
1. 使用合适的预训练数据 ：
   
   - 替换 pretrain_data 为更大的文本语料库（例如 Wikipedia 或 BooksCorpus 的子集），确保数据量足够支持模型学习。
   - 确保预训练数据格式为纯文本，而非带标签的结构化数据。
2. 调整训练参数 ：
   
   - 减小初始学习率（例如 --pretrain_lr 5e-5 ），避免模型参数波动过大。
   - 增加 warmup_steps （例如 --warmup_steps 100 ），让学习率更平滑地上升。
3. 修复日志重复问题 ：
   
   - 检查 run_llama.py 中 pretrain 函数的日志输出逻辑，确保每个 epoch 只打印一次损失和学习率。
### 4. 验证模型文件大小







####
### "Loaded 1 chunks" 的含义
- 数据分块逻辑 ： PretrainDataset 会将整个预训练文件读入内存，编码为 tokens 后，按照 max_seq_len （默认 1024）分割成多个 数据块（chunks） 。
- 分块条件 ：每个 chunk 至少需要包含 2 个 tokens 才会被保留。
- 为什么只有 1 个 chunk ： data/sst-train.txt 是一个情感分析数据集（格式为 label ||| sentence ），总文本量较小。当整个文件被编码为 tokens 后，总长度可能不足 1024，因此只生成了 1 个 chunk。
### 为什么这会导致问题
1. 数据量严重不足 ：自回归预训练需要大量文本数据来学习语言的统计规律，仅 1 个 chunk 意味着模型在每个 epoch 都重复看到相同的少量数据。
2. 模型无法有效学习 ：有限的数据会导致模型过拟合，无法泛化到新文本，这也是训练过程中损失值突变的原因之一。
3. 训练不稳定 ：由于数据量过小，模型参数更新会非常剧烈，容易出现损失值的突然跳变.



